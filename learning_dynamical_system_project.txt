Tasks that everyone has to do:
-read the two papers and write down questions to discuss (any question is very important)
-understand the relation between c_t (value of ambiguity set) and K_t (kalman gain)

Sasa obradovic:
-translation of Zorzi's code

Tommaso tubaldo:
-understand kalmannet code and see what can be reused for our purpose


Riccardo Zattra:
-understand kalmannet code and see what can be reused for our purpose


Question to be answered:
3) Relation between the C_t and K_t kalman gain (For me (ricky) I think that algorithm 1 at pag.3 of Zorzi's paper present the relation that we are looking for)
4) line 54 file parameters.py of folder lorenz attractor what is " ### Auxiliar MultiDimensional Tensor B and C (they make A --> Differential equation matrix)"?
5) check line 114 file main_lor_DT.py what the variable sys_model_pass2 does?
6) Which are the features relevant for the training of the DNN module? (starting from the 4 adopted in KNet, see pag.5 sec. B)
   Since in practice the state of the system is not always available, can we avoid using it as feature as suggested by Zorzi?
7) Which structure do we adopt for the DNN? Do we need a recursive formulation?
   Possible architectures: -the first architecture of KNet, i.e. an RNN with one or multiple GRU stages (see Fig.3 of the paper). In such
                             a way we exploit the recursive formulation of the Kalman Gain.
                           -a Feedforward NN from scratch
