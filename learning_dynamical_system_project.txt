Tasks that everyone has to do:
-read the two papers and write down questions to discuss (any question is very important)
-understand the relation between c_t (value of ambiguity set) and K_t (kalman gain)

Sasa obradovic:
-translation of Zorzi's code

Tommaso tubaldo:
-understand kalmannet code and see what can be reused for our purpose


Riccardo Zattra:
-understand kalmannet code and see what can be reused for our purpose


TODO:
1) Mettere if statement per ogni variabile interessata in base al fatto che si usi la rete neurale o no (classe RobustKalman()) - SASA - Done/There is not much needed to change here
2) Inserire modello sysnthetic NL model ( non cambiare nomi nel file parameters.py) + capire se il jacobiano numerico dÃ  problemi (se si calcolarlo a mano) - SASA - Done/ The jacobian changes absolutely nothing 
3) Trovare un allenamento della rete che va bene (provare ReLU al posto di Sigmoid sull'ultimo layer) - Tommaso 
4) Fare plot vari e calcolare inference time (MSE, Inference Time, Loss, State Pred vs State Ground Truth) - Tommaso
5) Pulire codice da commenti che non fanno nulla - SASA - Done
6) Riprovare codice KalmanNet con synthetic NL model e aggiungere un paio di grafici anche per quello - Tommaso
7) Cambiare parametri REKF 
8) Fare una riunione con Zorzi - Tutti insieme

MATLAB CODE FOR THE HARD CODED JACOBIAN OF THE SYNTH NON-LIN MODEL
%% Needs the symbolic math toolbox
syms x1 x2 alpha beta phi delta a b c


f(x1, x2) = alpha*sin(beta*[x1; x2] + phi) + delta
f_jac = jacobian(f(x1, x2), [x1, x2])

h(x1, x2) = a * ( b* [x1; x2] + c).^2 
h_jac = jacobian(h(x1, x2), [x1, x2])


###  Update Meeting 10-03-2025  ###
TO_DO: 1) Implement comparison of the three set of features: {F1,F2,F4}, {F1,F3,F4} and {F1,F2,F3,F4}
       2) Refine the test procedure by evaluating ~20 test iterations and then taking the average of such result
       3) Implement the comparison of Hard-coded VS Numerical Jacobian computation
       4) Try to change number of hidden layers and neurons


###  Results  ###
1) Compare input features:
       {F2} -> MSE: 0.0011 Computational Time: 0.1142
       {F1,F2,F4} -> MSE: 0.0011 Computational Time: 0.1120
       {F1,F3,F4} -> MSE: 0.0011 Computational Time: 0.1457
       {F1,F2,F3,F4} -> MSE: 0.0011 Computational Time: 0.1455


###  Update Meeting 26-03-2025  ###
TO_DO: - Test out RT-KalmanNet vs KalmanNet, by evaluating both set of features available ({F1,F2,F4}, {F1,F3,F4})** on
         the Synthetic NL model and the Lorenz attractor.
       - Evaluate MSE, std_dev of MSE, comp_time, std_dev of comp_time
       - Plot c_t realizations, one for each model
       - Report the results on a one page presentation, displaying the numerical results on a table and the realizations
         on plots
       - It is requested a code explanation too (How do we arrange it?)
         Explain the following files: 'main_robust_KNet.ipynb', 'robust_kalman.py', 'RT_KalmanNet_nn.py', 'Synthetic_NL_model/parameters.py'


** the actual set of features implemented on KalmanNet are {F1,F2,F3,F4}, thus we must compare with only this set of features


Updated TO_DO:  Sasa -> Implement and test Lorenz Attractor for both models
                Tommi -> Evaluate MSE, ... using pandas to save them (additionally test different Q and R to test out the robustness of RT_KNet)
                Ricky -> Write the one page presentation with two main tables, one for each ss model, each composed by two main colums, one for KNet and one for RT_KNet.
                         Add the plots for the c_t realizations too.(tabella fatta)
